{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d733a94a-777e-4fa9-a2d4-69863d669f79",
   "metadata": {},
   "source": [
    "### Objective: Explore the relationship between trader performance and market sentiment; surface actionable insights for trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8e1dac-f3ff-4178-b368-3745ae1de17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405987d-4e2e-4187-a689-463e17409abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"assignment folder\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6eeecf6-6583-4fd0-802d-321d759aa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_read_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def normalize_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df.columns = [c.replace(\" \", \"_\").replace(\"-\", \"_\") for c in df.columns]\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def find_datetime_col(df, candidates):\n",
    "    for name in candidates:\n",
    "        if name in df.columns:\n",
    "            # try parse sample\n",
    "            try:\n",
    "                tmp = pd.to_datetime(df[name], errors='coerce')\n",
    "                if tmp.notna().any():\n",
    "                    return name\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def ensure_utc(dt_series):\n",
    "    # converting to datetime and force UTC tz-aware where possible\n",
    "    s = pd.to_datetime(dt_series, errors='coerce')\n",
    "    # if already tz-aware, converting to UTC\n",
    "    if s.dt.tz is not None:\n",
    "        s = s.dt.tz_convert(\"UTC\")\n",
    "    else:\n",
    "        # localizing naive to UTC \n",
    "        s = s.dt.tz_localize(\"UTC\")\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1e312-da0d-46a3-a523-da8172f7b5c6",
   "metadata": {},
   "source": [
    "####  1) Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3e8a8d3-8780-4a1f-91b8-16c2fa3f9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades=pd.read_csv(r\"C:\\Users\\saram\\Downloads\\historical_data.csv\")\n",
    "sent=pd.read_csv(r\"C:\\Users\\saram\\Downloads\\fear_greed_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3c6fcca-1008-45eb-8570-fce630e22d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Coin</th>\n",
       "      <th>Execution Price</th>\n",
       "      <th>Size Tokens</th>\n",
       "      <th>Size USD</th>\n",
       "      <th>Side</th>\n",
       "      <th>Timestamp IST</th>\n",
       "      <th>Start Position</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Closed PnL</th>\n",
       "      <th>Transaction Hash</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Crossed</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Trade ID</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9769</td>\n",
       "      <td>986.87</td>\n",
       "      <td>7872.16</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.345404</td>\n",
       "      <td>8.950000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9800</td>\n",
       "      <td>16.00</td>\n",
       "      <td>127.68</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>986.524596</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>4.430000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9855</td>\n",
       "      <td>144.09</td>\n",
       "      <td>1150.63</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1002.518996</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>6.600000e+14</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9874</td>\n",
       "      <td>142.98</td>\n",
       "      <td>1142.04</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1146.558564</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050043</td>\n",
       "      <td>1.080000e+15</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xae5eacaf9c6b9111fd53034a602c192a04e082ed</td>\n",
       "      <td>@107</td>\n",
       "      <td>7.9894</td>\n",
       "      <td>8.73</td>\n",
       "      <td>69.75</td>\n",
       "      <td>BUY</td>\n",
       "      <td>02-12-2024 22:50</td>\n",
       "      <td>1289.488521</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0xec09451986a1874e3a980418412fcd0201f500c95bac...</td>\n",
       "      <td>52017706630</td>\n",
       "      <td>True</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>1.050000e+15</td>\n",
       "      <td>1.730000e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Account  Coin  Execution Price  \\\n",
       "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769   \n",
       "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800   \n",
       "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855   \n",
       "3  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9874   \n",
       "4  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9894   \n",
       "\n",
       "   Size Tokens  Size USD Side     Timestamp IST  Start Position Direction  \\\n",
       "0       986.87   7872.16  BUY  02-12-2024 22:50        0.000000       Buy   \n",
       "1        16.00    127.68  BUY  02-12-2024 22:50      986.524596       Buy   \n",
       "2       144.09   1150.63  BUY  02-12-2024 22:50     1002.518996       Buy   \n",
       "3       142.98   1142.04  BUY  02-12-2024 22:50     1146.558564       Buy   \n",
       "4         8.73     69.75  BUY  02-12-2024 22:50     1289.488521       Buy   \n",
       "\n",
       "   Closed PnL                                   Transaction Hash     Order ID  \\\n",
       "0         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630   \n",
       "1         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630   \n",
       "2         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630   \n",
       "3         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630   \n",
       "4         0.0  0xec09451986a1874e3a980418412fcd0201f500c95bac...  52017706630   \n",
       "\n",
       "   Crossed       Fee      Trade ID     Timestamp  \n",
       "0     True  0.345404  8.950000e+14  1.730000e+12  \n",
       "1     True  0.005600  4.430000e+14  1.730000e+12  \n",
       "2     True  0.050431  6.600000e+14  1.730000e+12  \n",
       "3     True  0.050043  1.080000e+15  1.730000e+12  \n",
       "4     True  0.003055  1.050000e+15  1.730000e+12  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trades.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4978b61b-176a-4af6-bcf3-241ec571820d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>classification</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1517463000</td>\n",
       "      <td>30</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1517549400</td>\n",
       "      <td>15</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1517635800</td>\n",
       "      <td>40</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2018-02-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1517722200</td>\n",
       "      <td>24</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1517808600</td>\n",
       "      <td>11</td>\n",
       "      <td>Extreme Fear</td>\n",
       "      <td>2018-02-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  value classification        date\n",
       "0  1517463000     30           Fear  2018-02-01\n",
       "1  1517549400     15   Extreme Fear  2018-02-02\n",
       "2  1517635800     40           Fear  2018-02-03\n",
       "3  1517722200     24   Extreme Fear  2018-02-04\n",
       "4  1517808600     11   Extreme Fear  2018-02-05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e67d9-4bda-4e05-85bf-09fb1a4120f4",
   "metadata": {},
   "source": [
    "#### 2) Normalize column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d68c93-38fa-4297-985e-05de4d814948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cols(df):\n",
    "    \"\"\"Normalizes column names by:\n",
    "    - stripping extra spaces\n",
    "    - converting to lowercase\n",
    "    - replacing spaces with underscores\"\"\"\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_')\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a060a9-9c1e-43fb-8469-582a61b5b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = normalize_cols(trades)\n",
    "sent = normalize_cols(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fa218-9c07-45bb-ab29-12afea45f2ba",
   "metadata": {},
   "source": [
    "#### 3) Data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12f27a7e-8cb3-4c89-89b1-bf5874ab5296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Trades DQ ---\n",
      "rows: 211224\n",
      "columns: 16\n",
      "duplicates: 0\n",
      "missing per column (top 10):\n",
      "account            0\n",
      "coin               0\n",
      "execution_price    0\n",
      "size_tokens        0\n",
      "size_usd           0\n",
      "side               0\n",
      "timestamp_ist      0\n",
      "start_position     0\n",
      "direction          0\n",
      "closed_pnl         0\n",
      "dtype: int64\n",
      "\n",
      "--- Sentiment DQ ---\n",
      "rows: 2644\n",
      "columns: 4\n",
      "duplicates: 0\n",
      "missing per column (top 10):\n",
      "timestamp         0\n",
      "value             0\n",
      "classification    0\n",
      "date              0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dqa_print(df, name):\n",
    "    print(f\"--- {name} DQ ---\")\n",
    "    print(\"rows:\", len(df))\n",
    "    print(\"columns:\", len(df.columns))\n",
    "    print(\"duplicates:\", int(df.duplicated().sum()))\n",
    "    print(\"missing per column (top 10):\")\n",
    "    print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "    print()\n",
    "\n",
    "dqa_print(trades, \"Trades\")\n",
    "dqa_print(sent, \"Sentiment\")\n",
    "\n",
    "\n",
    "pd.DataFrame({\"column\":list(trades.columns), \"missing\":trades.isna().sum().values, \"dtype\":trades.dtypes.astype(str).values}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR,\"trades_columns_report.csv\"), index=False)\n",
    "pd.DataFrame({\"column\":list(sent.columns), \"missing\":sent.isna().sum().values, \"dtype\":sent.dtypes.astype(str).values}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR,\"sent_columns_report.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0bc8a-bf39-496c-8d64-657e76c45f0c",
   "metadata": {},
   "source": [
    "#### 4) Detect and convert timestamps to UTC tz-aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62cf4337-d17d-4b4c-a593-219eb5571a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_datetime_col(df, candidates):\n",
    "    \"\"\"Finds the first column in df whose name matches one in candidates.\n",
    "    Matching is case-insensitive.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if col.lower() in [c.lower() for c in candidates]:\n",
    "            return col\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6241c560-e0ba-4919-a4f8-585b7cb5a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_utc(series):\n",
    "    \"\"\"Converts a pandas Series to timezone-aware UTC datetime.\"\"\"\n",
    "    # Convert to datetime if not already\n",
    "    series = pd.to_datetime(series, errors='coerce', utc=True)\n",
    "\n",
    "    # If already timezone-naive, localize to UTC\n",
    "    if series.dt.tz is None:\n",
    "        series = series.dt.tz_localize('UTC')\n",
    "    else:\n",
    "        series = series.dt.tz_convert('UTC')\n",
    "\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36218d52-2364-4750-b76f-18f3463dc2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected time columns: timestamp timestamp\n"
     ]
    }
   ],
   "source": [
    "trade_time_candidates = [\"time\",\"timestamp\",\"date\",\"datetime\",\"trade_time\"]\n",
    "sent_time_candidates = [\"date\",\"datetime\",\"timestamp\",\"time\"]\n",
    "\n",
    "trade_time_col = find_datetime_col(trades, trade_time_candidates)\n",
    "sent_time_col = find_datetime_col(sent, sent_time_candidates)\n",
    "\n",
    "if trade_time_col is None or sent_time_col is None:\n",
    "    raise RuntimeError(f\"Could not detect time columns. Detected trade_time_col={trade_time_col}, sent_time_col={sent_time_col}\")\n",
    "\n",
    "print(\"Detected time columns:\", trade_time_col, sent_time_col)\n",
    "\n",
    "# convert to tz-aware UTC\n",
    "trades[trade_time_col] = ensure_utc(trades[trade_time_col])\n",
    "sent[sent_time_col] = ensure_utc(sent[sent_time_col])\n",
    "\n",
    "# drop rows missing times\n",
    "trades = trades[~trades[trade_time_col].isna()].copy()\n",
    "sent = sent[~sent[sent_time_col].isna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26522-e603-4ce9-a469-8d0539e4e167",
   "metadata": {},
   "source": [
    "#### 5) Normalize numeric columns and identify key fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2efaf697-9da2-4d12-a18e-adf8a1edf571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: pnl= closed_pnl size= size_tokens price= execution_price side= side account= account symbol= coin leverage= _leverage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\3184332210.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  trades[\"_return_pct\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# heuristics for column names\n",
    "def find_first(cols, patterns):\n",
    "    for p in patterns:\n",
    "        if p in cols:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "cols = trades.columns.tolist()\n",
    "# detect PnL-like column\n",
    "pnl_col = next((c for c in cols if \"pnl\" in c.lower() or \"profit\" in c.lower()), None)\n",
    "size_col = next((c for c in cols if any(x==c for x in [\"size\",\"qty\",\"quantity\",\"volume\",\"amount\"]) or \"size\" in c.lower()), None)\n",
    "price_col = next((c for c in cols if any(x in c.lower() for x in [\"price\",\"execution_price\",\"exec_price\",\"entry_price\",\"exit_price\"])), None)\n",
    "side_col = next((c for c in cols if c.lower() in [\"side\",\"direction\",\"buy_sell\",\"buy/sell\"] or \"side\" in c.lower()), None)\n",
    "account_col = next((c for c in cols if any(x in c.lower() for x in [\"account\",\"acct\",\"client\",\"trader\",\"user\"])), None)\n",
    "symbol_col = next((c for c in cols if any(x in c.lower() for x in [\"symbol\",\"coin\",\"pair\",\"instrument\"])), None)\n",
    "leverage_col = next((c for c in cols if \"leverage\" in c.lower() or \"lev\"==c.lower()), None)\n",
    "\n",
    "print(\"Detected: pnl=\", pnl_col, \"size=\", size_col, \"price=\", price_col, \"side=\", side_col, \"account=\", account_col, \"symbol=\", symbol_col, \"leverage=\", leverage_col)\n",
    "# Coerce to numeric engineered columns\n",
    "trades[\"_pnl\"] = pd.to_numeric(trades[pnl_col], errors='coerce') if pnl_col else np.nan\n",
    "trades[\"_size\"] = pd.to_numeric(trades[size_col], errors='coerce') if size_col else np.nan\n",
    "trades[\"_price\"] = pd.to_numeric(trades[price_col], errors='coerce') if price_col else np.nan\n",
    "trades[\"_leverage\"] = pd.to_numeric(trades[leverage_col], errors='coerce') if leverage_col else np.nan\n",
    "\n",
    "# side normalization\n",
    "if side_col:\n",
    "    trades[\"_side\"] = trades[side_col].astype(str).str.lower().str.strip()\n",
    "else:\n",
    "    trades[\"_side\"] = np.nan\n",
    "\n",
    "# notional and return\n",
    "trades[\"_notional\"] = trades[\"_size\"] * trades[\"_price\"]\n",
    "trades[\"_return_pct\"] = trades[\"_pnl\"] / trades[\"_notional\"]\n",
    "trades[\"_return_pct\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "trades[\"_profitable\"] = trades[\"_pnl\"] > 0\n",
    "\n",
    "# holding time if open & close timestamps exist\n",
    "open_candidates = [c for c in cols if \"open_time\" in c or \"open\"==c or \"start_time\" in c]\n",
    "close_candidates = [c for c in cols if \"close_time\" in c or \"close\"==c or \"end_time\" in c]\n",
    "open_time_col = open_candidates[0] if open_candidates else None\n",
    "close_time_col = close_candidates[0] if close_candidates else None\n",
    "if open_time_col and close_time_col and open_time_col in trades.columns and close_time_col in trades.columns:\n",
    "    trades[open_time_col] = ensure_utc(trades[open_time_col])\n",
    "    trades[close_time_col] = ensure_utc(trades[close_time_col])\n",
    "    trades[\"_holding_time_sec\"] = (trades[close_time_col] - trades[open_time_col]).dt.total_seconds()\n",
    "else:\n",
    "    trades[\"_holding_time_sec\"] = np.nan\n",
    "\n",
    "# saving a small sample of engineered trades\n",
    "trades[[trade_time_col,\"_pnl\",\"_size\",\"_price\",\"_notional\",\"_return_pct\",\"_profitable\",\"_leverage\",\"_side\"]].head(20).to_csv(\n",
    "    os.path.join(OUTPUT_DIR,\"trades_engineered_sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f56d1ca9-ef97-4b07-b873-55a2d29addcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect trade timestamp column\n",
    "possible_trade_time_cols = [\"timestamp\", \"time\", \"trade_time\", \"datetime\", \"date\"]\n",
    "trade_time_col = next((col for col in possible_trade_time_cols if col in trades.columns), None)\n",
    "\n",
    "if trade_time_col is None:\n",
    "    raise ValueError(f\"No trade time column found in trades. Columns found: {trades.columns.tolist()}\")\n",
    "\n",
    "trades[trade_time_col] = ensure_utc(trades[trade_time_col])\n",
    "\n",
    "# saving a small sample of engineered trades\n",
    "trades[[trade_time_col, \"_pnl\", \"_size\", \"_price\", \"_notional\", \"_return_pct\", \"_profitable\", \"_leverage\", \"_side\"]] \\\n",
    "    .head(20) \\\n",
    "    .to_csv(os.path.join(OUTPUT_DIR, \"trades_engineered_sample.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b101a-92c4-4ec4-9f58-f74df88c4d8e",
   "metadata": {},
   "source": [
    "#### 6) Prepare sentiment numeric & classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "314b834b-fd5b-47b9-b4f4-fef48157bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sentiment value column: value\n"
     ]
    }
   ],
   "source": [
    "possible_sent_val_cols = [c for c in sent.columns if any(k in c for k in [\"value\",\"index\",\"score\",\"val\",\"fear\",\"greed\"])]\n",
    "sent_val_col = None\n",
    "for c in possible_sent_val_cols:\n",
    "    sent[c] = pd.to_numeric(sent[c], errors='coerce')\n",
    "    if sent[c].notna().any():\n",
    "        sent_val_col = c\n",
    "        break\n",
    "if sent_val_col is None:\n",
    "    # fallback\n",
    "    for c in sent.columns:\n",
    "        if pd.api.types.is_numeric_dtype(sent[c]):\n",
    "            sent_val_col = c\n",
    "            break\n",
    "if sent_val_col is None:\n",
    "    raise RuntimeError(\"No numeric sentiment value column found in sentiment dataset.\")\n",
    "print(\"Using sentiment value column:\", sent_val_col)\n",
    "\n",
    "possible_class_cols = [c for c in sent.columns if any(k in c for k in [\"class\",\"classification\",\"label\"])]\n",
    "sent_class_col = possible_class_cols[0] if possible_class_cols else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd7858-e98e-42b9-8488-df4f5a98ed6f",
   "metadata": {},
   "source": [
    "#### 7) Align sentiment with trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae1d9906-9717-4870-8137-d428905f6ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment class column: classification\n"
     ]
    }
   ],
   "source": [
    "possible_class_cols = [\"classification\", \"category\", \"sentiment_class\", \"label\"]\n",
    "sent_class_col = None\n",
    "for col in sent.columns:\n",
    "    if col.lower() in possible_class_cols:\n",
    "        sent_class_col = col\n",
    "        break\n",
    "\n",
    "print(f\"Sentiment class column: {sent_class_col if sent_class_col else 'None found'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de2a4738-87ec-498e-8a9e-cb020a78205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment mapping mode: daily (median/day: 1.0)\n",
      "Mapped sentiment to trades: 0 trades have sentiment value\n",
      "                                      account  coin  execution_price  \\\n",
      "0  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9769   \n",
      "1  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9800   \n",
      "2  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9855   \n",
      "3  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9874   \n",
      "4  0xae5eacaf9c6b9111fd53034a602c192a04e082ed  @107           7.9894   \n",
      "\n",
      "   size_tokens  size_usd side     timestamp_ist  start_position direction  \\\n",
      "0       986.87   7872.16  BUY  02-12-2024 22:50        0.000000       Buy   \n",
      "1        16.00    127.68  BUY  02-12-2024 22:50      986.524596       Buy   \n",
      "2       144.09   1150.63  BUY  02-12-2024 22:50     1002.518996       Buy   \n",
      "3       142.98   1142.04  BUY  02-12-2024 22:50     1146.558564       Buy   \n",
      "4         8.73     69.75  BUY  02-12-2024 22:50     1289.488521       Buy   \n",
      "\n",
      "   closed_pnl  ...  _price  _leverage  _side    _notional  _return_pct  \\\n",
      "0         0.0  ...  7.9769        NaN    buy  7872.163303          0.0   \n",
      "1         0.0  ...  7.9800        NaN    buy   127.680000          0.0   \n",
      "2         0.0  ...  7.9855        NaN    buy  1150.630695          0.0   \n",
      "3         0.0  ...  7.9874        NaN    buy  1142.038452          0.0   \n",
      "4         0.0  ...  7.9894        NaN    buy    69.747462          0.0   \n",
      "\n",
      "  _profitable  _holding_time_sec                _trade_day  _s_val_num  \\\n",
      "0       False                NaN 1970-01-01 00:00:00+00:00         NaN   \n",
      "1       False                NaN 1970-01-01 00:00:00+00:00         NaN   \n",
      "2       False                NaN 1970-01-01 00:00:00+00:00         NaN   \n",
      "3       False                NaN 1970-01-01 00:00:00+00:00         NaN   \n",
      "4       False                NaN 1970-01-01 00:00:00+00:00         NaN   \n",
      "\n",
      "   _s_bucket  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "possible_date_cols = [\"Date\", \"date\", \"timestamp\", \"time\", \"datetime\"]\n",
    "sent_time_col = next((col for col in possible_date_cols if col in sent.columns), None)\n",
    "if sent_time_col is None:\n",
    "    raise ValueError(f\"No date column found in sentiment data. Found: {sent.columns.tolist()}\")\n",
    "\n",
    "possible_val_cols = [\"Classification\", \"value\", \"sentiment\", \"score\"]\n",
    "sent_val_col = next((col for col in possible_val_cols if col in sent.columns), None)\n",
    "if sent_val_col is None:\n",
    "    raise ValueError(f\"No sentiment value column found. Found: {sent.columns.tolist()}\")\n",
    "\n",
    "trade_time_col = next((col for col in possible_date_cols if col in trades.columns), None)\n",
    "if trade_time_col is None:\n",
    "    raise ValueError(f\"No trade time column found in trades data. Found: {trades.columns.tolist()}\")\n",
    "\n",
    "# --- Convert datetime ---\n",
    "sent[sent_time_col] = pd.to_datetime(sent[sent_time_col], errors='coerce', utc=True)\n",
    "trades[trade_time_col] = pd.to_datetime(trades[trade_time_col], errors='coerce', utc=True)\n",
    "\n",
    "# --- Sort sentiment ---\n",
    "sent = sent.sort_values(by=sent_time_col).reset_index(drop=True)\n",
    "sent_index = sent.set_index(sent_time_col)\n",
    "sent_indexed = sent_index[[sent_val_col]].copy()\n",
    "\n",
    "# --- Detect frequency ---\n",
    "try:\n",
    "    median_per_day = sent_indexed.resample(\"D\").count().squeeze().median()\n",
    "except Exception:\n",
    "    median_per_day = 1\n",
    "mapping_mode = \"daily\" if median_per_day <= 2 else \"time_series\"\n",
    "print(f\"Sentiment mapping mode: {mapping_mode} (median/day: {median_per_day})\")\n",
    "\n",
    "# --- Ensure UTC ---\n",
    "if sent_indexed.index.tz is None:\n",
    "    sent_indexed.index = sent_indexed.index.tz_localize(\"UTC\")\n",
    "else:\n",
    "    sent_indexed.index = sent_indexed.index.tz_convert(\"UTC\")\n",
    "\n",
    "sent_indexed = sent_indexed.reset_index().rename(columns={\n",
    "    sent_time_col: \"_s_timestamp\",\n",
    "    sent_val_col: \"_s_val\"\n",
    "})\n",
    "\n",
    "# --- Convert sentiment classification to numeric if needed ---\n",
    "if sent_indexed[\"_s_val\"].dtype == object:\n",
    "    mapping = {\"Fear\": 25, \"Neutral\": 50, \"Greed\": 75}  # Example mapping\n",
    "    sent_indexed[\"_s_val_num\"] = sent_indexed[\"_s_val\"].map(mapping)\n",
    "else:\n",
    "    sent_indexed[\"_s_val_num\"] = sent_indexed[\"_s_val\"]\n",
    "\n",
    "# --- Map trades to sentiment ---\n",
    "if mapping_mode == \"time_series\":\n",
    "    sent_times = sent_indexed[\"_s_timestamp\"].values\n",
    "    def map_prev_sent(trade_ts):\n",
    "        if pd.isna(trade_ts):\n",
    "            return pd.NaT\n",
    "        idx = np.searchsorted(sent_times, trade_ts, side='right') - 1\n",
    "        return sent_times[idx] if idx >= 0 else pd.NaT\n",
    "    trades[\"_sent_time_mapped\"] = trades[trade_time_col].apply(map_prev_sent)\n",
    "    trades[\"_sent_time_mapped\"] = pd.to_datetime(trades[\"_sent_time_mapped\"], utc=True)\n",
    "    trades = trades.merge(\n",
    "        sent_indexed.rename(columns={\"_s_timestamp\": \"_s_time\"}),\n",
    "        left_on=\"_sent_time_mapped\", right_on=\"_s_time\", how=\"left\"\n",
    "    )\n",
    "else:\n",
    "    trades[\"_trade_day\"] = trades[trade_time_col].dt.floor(\"D\")\n",
    "    sent_indexed[\"_sent_day\"] = sent_indexed[\"_s_timestamp\"].dt.floor(\"D\")\n",
    "    sent_by_day = sent_indexed.groupby(\"_sent_day\")[\"_s_val_num\"].mean().reset_index()\n",
    "    trades = trades.merge(sent_by_day, left_on=\"_trade_day\", right_on=\"_sent_day\", how=\"left\")\n",
    "    trades[\"_s_val_num\"] = trades[\"_s_val_num\"].fillna(trades[\"_s_val_num\"].median())\n",
    "    trades.drop(columns=[\"_sent_day\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "mapped_count = trades[\"_s_val_num\"].notna().sum()\n",
    "print(f\"Mapped sentiment to trades: {mapped_count} trades have sentiment value\")\n",
    "\n",
    "# --- Create sentiment bucket ---\n",
    "trades[\"_s_bucket\"] = pd.cut(\n",
    "    trades[\"_s_val_num\"],\n",
    "    bins=[-1, 33, 66, 101],\n",
    "    labels=[\"fear\", \"neutral\", \"greed\"]\n",
    ")\n",
    "\n",
    "print(trades.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4cee0-f549-429d-82c4-3ed1ff11a8b2",
   "metadata": {},
   "source": [
    "#### 8) Market features: simple price returns & volatility (if price exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01751f-84dc-4d15-a164-b675aad6adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"_price\" in trades.columns and trades[\"_price\"].notna().sum() > 0:\n",
    "    # Make sure trade_time_col exists\n",
    "    possible_trade_time_cols = [\"timestamp\", \"time\", \"trade_time\", \"datetime\"]\n",
    "    trade_time_col = next((col for col in possible_trade_time_cols if col in trades.columns), None)\n",
    "    if trade_time_col is None:\n",
    "        raise ValueError(f\"No trade time column found in trades. Columns found: {trades.columns.tolist()}\")\n",
    "\n",
    "    # Ensure datetime format\n",
    "    trades[trade_time_col] = pd.to_datetime(trades[trade_time_col])\n",
    "\n",
    "    # Resample to hourly median prices\n",
    "    price_hourly = (\n",
    "        trades.set_index(trade_time_col)\n",
    "        .resample(\"1H\")[\"_price\"]\n",
    "        .median()\n",
    "        .ffill()\n",
    "        .to_frame(name=\"median_price\")\n",
    "    )\n",
    "    price_hourly[\"log_return\"] = np.log(price_hourly[\"median_price\"]).diff()\n",
    "    price_hourly[\"volatility_24h\"] = price_hourly[\"log_return\"].rolling(24).std() * (24 ** 0.5)\n",
    "    price_hourly.reset_index(inplace=True)\n",
    "    price_hourly.to_csv(os.path.join(OUTPUT_DIR, \"price_hourly.csv\"), index=False)\n",
    "\n",
    "    # Convert both to same dtype (datetime64)\n",
    "    price_times = price_hourly[trade_time_col].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "    def map_price_feat(trade_ts):\n",
    "        if pd.isna(trade_ts):\n",
    "            return np.nan\n",
    "        ts = np.datetime64(trade_ts)  # ensure same type\n",
    "        idx = np.searchsorted(price_times, ts, side='right') - 1\n",
    "        if idx >= 0:\n",
    "            return price_hourly.iloc[idx][\"median_price\"]\n",
    "        return np.nan\n",
    "\n",
    "    trades[\"_mapped_median_price\"] = trades[trade_time_col].apply(map_price_feat)\n",
    "\n",
    "else:\n",
    "    price_hourly = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787108f-047b-40a9-8965-359c0d873d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"_price\" in trades.columns and trades[\"_price\"].notna().sum() > 0:\n",
    "    price_hourly = trades.set_index(trade_time_col).resample(\"1H\")[\"_price\"].median().ffill()\n",
    "    price_hourly = price_hourly.to_frame(name=\"median_price\")\n",
    "    price_hourly[\"log_return\"] = np.log(price_hourly[\"median_price\"]).diff()\n",
    "    price_hourly[\"volatility_24h\"] = price_hourly[\"log_return\"].rolling(24).std() * (24 ** 0.5)\n",
    "    price_hourly.reset_index(inplace=True)\n",
    "    price_hourly.to_csv(os.path.join(OUTPUT_DIR,\"price_hourly.csv\"), index=False)\n",
    "    price_times = price_hourly[trade_time_col].values\n",
    "    def map_price_feat(trade_ts):\n",
    "        if pd.isna(trade_ts):\n",
    "            return np.nan\n",
    "        idx = np.searchsorted(price_times, trade_ts, side='right') - 1\n",
    "        if idx >= 0:\n",
    "            return price_hourly.iloc[idx][\"median_price\"]\n",
    "        return np.nan\n",
    "    trades[\"_mapped_median_price\"] = trades[trade_time_col].apply(map_price_feat)\n",
    "else:\n",
    "    price_hourly = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fabb2-c96c-4107-9805-19b8a60cb587",
   "metadata": {},
   "source": [
    "#### 9) Trader-level / rolling window features (if account exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca97ac7-76bd-4446-80ff-3a3fd00a02ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\1183215291.py:14: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  .resample(\"1H\")[\"_price\"]\n",
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\1183215291.py:30: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  ts = np.datetime64(trade_ts)  # ensure same type\n"
     ]
    }
   ],
   "source": [
    "if \"_price\" in trades.columns and trades[\"_price\"].notna().sum() > 0:\n",
    "    # Make sure trade_time_col exists\n",
    "    possible_trade_time_cols = [\"timestamp\", \"time\", \"trade_time\", \"datetime\"]\n",
    "    trade_time_col = next((col for col in possible_trade_time_cols if col in trades.columns), None)\n",
    "    if trade_time_col is None:\n",
    "        raise ValueError(f\"No trade time column found in trades. Columns found: {trades.columns.tolist()}\")\n",
    "\n",
    "    # Ensure datetime format\n",
    "    trades[trade_time_col] = pd.to_datetime(trades[trade_time_col])\n",
    "\n",
    "    # Resample to hourly median prices\n",
    "    price_hourly = (\n",
    "        trades.set_index(trade_time_col)\n",
    "        .resample(\"1H\")[\"_price\"]\n",
    "        .median()\n",
    "        .ffill()\n",
    "        .to_frame(name=\"median_price\")\n",
    "    )\n",
    "    price_hourly[\"log_return\"] = np.log(price_hourly[\"median_price\"]).diff()\n",
    "    price_hourly[\"volatility_24h\"] = price_hourly[\"log_return\"].rolling(24).std() * (24 ** 0.5)\n",
    "    price_hourly.reset_index(inplace=True)\n",
    "    price_hourly.to_csv(os.path.join(OUTPUT_DIR, \"price_hourly.csv\"), index=False)\n",
    "\n",
    "    # Convert both to same dtype (datetime64)\n",
    "    price_times = price_hourly[trade_time_col].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "    def map_price_feat(trade_ts):\n",
    "        if pd.isna(trade_ts):\n",
    "            return np.nan\n",
    "        ts = np.datetime64(trade_ts)  # ensure same type\n",
    "        idx = np.searchsorted(price_times, ts, side='right') - 1\n",
    "        if idx >= 0:\n",
    "            return price_hourly.iloc[idx][\"median_price\"]\n",
    "        return np.nan\n",
    "\n",
    "    trades[\"_mapped_median_price\"] = trades[trade_time_col].apply(map_price_feat)\n",
    "\n",
    "else:\n",
    "    price_hourly = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9ed11ea-e524-4678-9b24-7aa071854115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\3138670492.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(compute_rolling_features)\n"
     ]
    }
   ],
   "source": [
    "# Ensure columns exist\n",
    "if account_col in trades.columns and trade_time_col in trades.columns:\n",
    "    # Make sure time column is datetime\n",
    "    trades[trade_time_col] = pd.to_datetime(trades[trade_time_col], errors=\"coerce\")\n",
    "\n",
    "    # Ensure account column consistent type\n",
    "    trades[account_col] = trades[account_col].astype(str)\n",
    "\n",
    "    # Sort by account and time\n",
    "    trades = trades.sort_values([account_col, trade_time_col])\n",
    "\n",
    "    def compute_rolling_features(df):\n",
    "        # Must have datetime index for time-based rolling\n",
    "        df = df.set_index(trade_time_col).sort_index()\n",
    "\n",
    "        df[\"_winrate_7d\"] = df[\"_profitable\"].rolling(\"7d\").mean()\n",
    "        df[\"_avg_pnl_7d\"] = df[\"_pnl\"].rolling(\"7d\").mean()\n",
    "        df[\"_pnl_std_7d\"] = df[\"_pnl\"].rolling(\"7d\").std()\n",
    "\n",
    "        # Avoid divide-by-zero\n",
    "        df[\"_sharpe_like_7d\"] = df[\"_avg_pnl_7d\"] / df[\"_pnl_std_7d\"].replace(0, np.nan)\n",
    "\n",
    "        df[\"_trades_7d\"] = df[\"_pnl\"].rolling(\"7d\").count()\n",
    "\n",
    "        return df.reset_index()\n",
    "\n",
    "    # Apply per account\n",
    "    trades = (\n",
    "        trades.groupby(account_col, group_keys=False)\n",
    "        .apply(compute_rolling_features)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"No account column or trade time column found; skipping trader-level rolling features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ae447-d651-483e-a672-661801ac54c0",
   "metadata": {},
   "source": [
    "#### 10) EDA: summary stats and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba35147b-26b1-4dd3-969c-2ab6717932ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "stats = trades[[\"_pnl\",\"_size\",\"_price\",\"_notional\",\"_return_pct\",\"_leverage\",\"_holding_time_sec\"]].describe().transpose()\n",
    "stats.to_csv(os.path.join(OUTPUT_DIR,\"trades_summary_stats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de32324e-82e5-4bbe-b78e-d39f7e09b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "def save_hist(series, fname, bins=100):\n",
    "    v = series.dropna()\n",
    "    if v.shape[0] == 0:\n",
    "        return\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(v, bins=bins)\n",
    "    plt.title(f\"Distribution: {fname}\")\n",
    "    plt.xlabel(\"value\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,fname))\n",
    "    plt.close()\n",
    "\n",
    "save_hist(trades[\"_pnl\"], \"hist_pnl.png\", bins=120)\n",
    "save_hist(trades[\"_size\"], \"hist_size.png\", bins=80)\n",
    "save_hist(trades[\"_return_pct\"], \"hist_return_pct.png\", bins=80)\n",
    "save_hist(trades[\"_leverage\"], \"hist_leverage.png\", bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "804bf89a-4d6b-483d-b1f0-cb874733c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping histogram for hist_leverage.png — no valid values.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Plot histograms\n",
    "def save_hist(series, fname, bins=100):\n",
    "    # Drop NaNs and infinities\n",
    "    v = series.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    if v.shape[0] == 0:\n",
    "        print(f\"Skipping histogram for {fname} — no valid values.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.hist(v, bins=bins, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "    plt.title(f\"Distribution: {fname}\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, fname))\n",
    "    plt.close()\n",
    "\n",
    "# Call histograms for engineered features\n",
    "save_hist(trades[\"_pnl\"], \"hist_pnl.png\", bins=120)\n",
    "save_hist(trades[\"_size\"], \"hist_size.png\", bins=80)\n",
    "save_hist(trades[\"_return_pct\"], \"hist_return_pct.png\", bins=80)\n",
    "save_hist(trades[\"_leverage\"], \"hist_leverage.png\", bins=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6845ecd0-a263-4707-bd68-0a40b7230a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-series daily aggregated PnL vs sentiment\n",
    "trades[\"_day\"] = trades[trade_time_col].dt.tz_convert(\"UTC\").dt.floor(\"D\")\n",
    "sent[\"_day\"] = sent[sent_time_col].dt.tz_convert(\"UTC\").dt.floor(\"D\")\n",
    "daily_pnl = trades.groupby(\"_day\")[\"_pnl\"].sum().rename(\"daily_pnl\").reset_index()\n",
    "daily_sent = sent.groupby(\"_day\")[sent_val_col].mean().rename(\"daily_sent\").reset_index()\n",
    "daily = pd.merge(daily_pnl, daily_sent, left_on=\"_day\", right_on=\"_day\", how=\"left\")\n",
    "daily.to_csv(os.path.join(OUTPUT_DIR,\"daily_pnl_sent.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "681754b6-28f4-4895-a576-a77a97011d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(daily[\"_day\"], daily[\"daily_pnl\"])\n",
    "plt.title(\"Daily aggregated PnL\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"daily_pnl\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,\"daily_pnl.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d650370-e9f8-4aed-b669-df9a4d588c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(daily[\"_day\"], daily[\"daily_sent\"])\n",
    "plt.title(\"Daily sentiment (mean)\")\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"daily_sent\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,\"daily_sent.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066ec34-d5cd-4514-8ac0-9b1cccc65b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped analyses: PnL & win-rate by sentiment bucket\n",
    "bucket_stats = trades.groupby(\"_s_bucket\").agg(\n",
    "    n_trades=(\"_pnl\",\"count\"),\n",
    "    total_pnl=(\"_pnl\",\"sum\"),\n",
    "    mean_pnl=(\"_pnl\",\"mean\"),\n",
    "    median_pnl=(\"_pnl\",\"median\"),\n",
    "    winrate=(\"_profitable\",\"mean\")\n",
    ").reset_index()\n",
    "bucket_stats.to_csv(os.path.join(OUTPUT_DIR,\"bucket_stats.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb69b05-b1ae-4f4e-b5b7-61274aa4ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-section: mean return vs sentiment by account (if account exists)\n",
    "if account_col:\n",
    "    acc_stats = trades.groupby(account_col).agg(\n",
    "        mean_return=(\"_return_pct\",\"mean\"),\n",
    "        mean_sent=(\"_s_val\",\"mean\"),\n",
    "        trades_count=(\"_pnl\",\"count\")\n",
    "    ).reset_index()\n",
    "    acc_stats.to_csv(os.path.join(OUTPUT_DIR,\"account_sent_return.csv\"), index=False)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(acc_stats[\"mean_sent\"].fillna(0), acc_stats[\"mean_return\"].fillna(0), s=10)\n",
    "    plt.title(\"Account mean return vs mean sentiment\")\n",
    "    plt.xlabel(\"mean_sent\")\n",
    "    plt.ylabel(\"mean_return\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR,\"account_sent_return_scatter.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8afc8-098f-43f3-99f7-c2e725027a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numeric features\n",
    "num_cols = [c for c in [\"_pnl\",\"_size\",\"_price\",\"_notional\",\"_return_pct\",\"_leverage\",\"_s_val\"] if c in trades.columns]\n",
    "corr = trades[num_cols].corr()\n",
    "corr.to_csv(os.path.join(OUTPUT_DIR,\"correlation_matrix.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918e57b-4934-46a9-a19a-fabd2974e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: Mann-Whitney U between fear and greed\n",
    "if \"_s_bucket\" in trades.columns:\n",
    "    fear = trades.loc[trades[\"_s_bucket\"]==\"fear\", \"_pnl\"].dropna()\n",
    "    greed = trades.loc[trades[\"_s_bucket\"]==\"greed\", \"_pnl\"].dropna()\n",
    "    if len(fear)>0 and len(greed)>0:\n",
    "        try:\n",
    "            mw_stat, mw_p = stats.mannwhitneyu(fear, greed, alternative='two-sided')\n",
    "            with open(os.path.join(OUTPUT_DIR,\"stat_test_mannwhitney.txt\"), \"w\") as f:\n",
    "                f.write(f\"MW U stat={mw_stat}, p={mw_p}\\n\")\n",
    "        except Exception as e:\n",
    "            with open(os.path.join(OUTPUT_DIR,\"stat_test_mannwhitney.txt\"), \"w\") as f:\n",
    "                f.write(\"Mann-Whitney failed: \" + str(e) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9ed90-4cba-4691-9cec-a75dca497e11",
   "metadata": {},
   "source": [
    "#### 11) Event study around extreme sentiment shifts (top/bottom 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4166eb07-de69-4ee9-b2fe-9d7390cbf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    high_cut = sent[sent_val_col].quantile(0.95)\n",
    "    low_cut  = sent[sent_val_col].quantile(0.05)\n",
    "    high_events = sent.loc[sent[sent_val_col] >= high_cut, sent_time_col].sort_values().tolist()\n",
    "    low_events  = sent.loc[sent[sent_val_col] <= low_cut, sent_time_col].sort_values().tolist()\n",
    "    # limit number of events to analyze for speed\n",
    "    high_events = high_events[:10]\n",
    "    low_events = low_events[:10]\n",
    "    def summarize_event_window(event_times, w_before=pd.Timedelta(days=3), w_after=pd.Timedelta(days=3)):\n",
    "        summaries = []\n",
    "        for et in event_times:\n",
    "            start = et - w_before\n",
    "            end = et + w_after\n",
    "            subset = trades[(trades[trade_time_col] >= start) & (trades[trade_time_col] <= end)]\n",
    "            summaries.append({\n",
    "                \"event_time\": et,\n",
    "                \"n_trades\": len(subset),\n",
    "                \"total_pnl\": float(subset[\"_pnl\"].sum()) if len(subset)>0 else 0.0,\n",
    "                \"mean_pnl\": float(subset[\"_pnl\"].mean()) if len(subset)>0 else 0.0\n",
    "            })\n",
    "        return pd.DataFrame(summaries)\n",
    "    he_df = summarize_event_window(high_events)\n",
    "    le_df = summarize_event_window(low_events)\n",
    "    he_df.to_csv(os.path.join(OUTPUT_DIR,\"event_high_summary.csv\"), index=False)\n",
    "    le_df.to_csv(os.path.join(OUTPUT_DIR,\"event_low_summary.csv\"), index=False)\n",
    "except Exception as e:\n",
    "    print(\"Event study failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61451c-ab47-4a32-8eef-440aca3c8c51",
   "metadata": {},
   "source": [
    "#### 12) Predictive modeling: predict whether a trade is profitable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8a7aa-685f-4623-be54-bd0e8cb0c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer _s_val, fallback to _s_val_num if needed\n",
    "sentiment_col = \"_s_val\" if \"_s_val\" in trades.columns else \"_s_val_num\" if \"_s_val_num\" in trades.columns else None\n",
    "\n",
    "# Base feature list\n",
    "features = [\"_size\", \"_price\", \"_notional\", \"_return_pct\", sentiment_col, \"_leverage\"]\n",
    "\n",
    "# Encode side as numeric if available\n",
    "if \"_side\" in trades.columns:\n",
    "    trades[\"_side_enc\"] = trades[\"_side\"].map(\n",
    "        lambda x: 1 if isinstance(x, str) and (\"buy\" in x.lower() or \"long\" in x.lower())\n",
    "        else (0 if isinstance(x, str) and (\"sell\" in x.lower() or \"short\" in x.lower())\n",
    "        else np.nan)\n",
    "    )\n",
    "    features.append(\"_side_enc\")\n",
    "\n",
    "# Drop None and ensure all features exist in trades\n",
    "features = [f for f in features if f is not None and f in trades.columns]\n",
    "\n",
    "# Keep only numeric features for modeling\n",
    "numeric_features = [f for f in features if pd.api.types.is_numeric_dtype(trades[f])]\n",
    "\n",
    "# Prepare dataset\n",
    "model_df = trades[numeric_features + [\"_profitable\", \"_pnl\"]].copy()\n",
    "model_df = model_df.rename(columns={\"_profitable\": \"profitable\", \"_pnl\": \"pnl\"})\n",
    "model_df = model_df.dropna(subset=[\"profitable\"])\n",
    "\n",
    "# Require both classes present\n",
    "if len(model_df[\"profitable\"].unique()) > 1 and len(model_df) > 100:\n",
    "    X = model_df[numeric_features].to_numpy()\n",
    "    y = model_df[\"profitable\"].astype(int).to_numpy()\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imputer.fit_transform(X)\n",
    "\n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imp)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(\"RF accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Permutation importance\n",
    "    perm = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    perm_df = pd.DataFrame({\n",
    "        \"feature\": numeric_features,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "    perm_df.to_csv(os.path.join(OUTPUT_DIR, \"permutation_importance.csv\"), index=False)\n",
    "\n",
    "    # Logistic Regression for interpretability\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    print(\"LR accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "    print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "    # Save models and transformers\n",
    "    joblib.dump(rf, os.path.join(OUTPUT_DIR, \"rf_profitable.joblib\"))\n",
    "    joblib.dump(lr, os.path.join(OUTPUT_DIR, \"lr_profitable.joblib\"))\n",
    "    joblib.dump(scaler, os.path.join(OUTPUT_DIR, \"scaler.joblib\"))\n",
    "    joblib.dump(imputer, os.path.join(OUTPUT_DIR, \"imputer.joblib\"))\n",
    "\n",
    "else:\n",
    "    print(\"Not enough data/classes to train model (need both classes and >100 rows).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f0673-de9b-4ab4-8f0b-997a21da2756",
   "metadata": {},
   "source": [
    "#### 13) Auto-insights (simple heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d88cfab-f0ae-45f3-b786-627ba55202a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\4287449336.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  wr = trades.groupby(\"_s_bucket\")[\"_profitable\"].mean().to_dict()\n"
     ]
    }
   ],
   "source": [
    "insights = []\n",
    "\n",
    "# Leverage vs PnL correlation\n",
    "if \"_leverage\" in trades.columns and trades[\"_leverage\"].notna().sum() > 10:\n",
    "    corr = trades[[\"_leverage\", \"_pnl\"]].dropna().corr().iloc[0, 1]\n",
    "    insights.append(f\"Correlation between leverage and PnL: {corr:.3f}\")\n",
    "\n",
    "# Win-rate by sentiment bucket\n",
    "if \"_s_bucket\" in trades.columns:\n",
    "    wr = trades.groupby(\"_s_bucket\")[\"_profitable\"].mean().to_dict()\n",
    "    insights.append(\"Win-rate by sentiment bucket: \" + \", \".join([f\"{k}:{v:.2%}\" for k, v in wr.items()]))\n",
    "\n",
    "# Sentiment mapping count — check for either _s_val or _s_val_num\n",
    "sentiment_col = \"_s_val\" if \"_s_val\" in trades.columns else \"_s_val_num\" if \"_s_val_num\" in trades.columns else None\n",
    "if sentiment_col:\n",
    "    mapped_count = trades[sentiment_col].notna().sum()\n",
    "    insights.append(f\"Mapped trades with sentiment: {int(mapped_count)} / {len(trades)}\")\n",
    "else:\n",
    "    insights.append(\"No sentiment values found in dataset.\")\n",
    "\n",
    "# Save insights\n",
    "pd.DataFrame({\"insight\": insights}).to_csv(os.path.join(OUTPUT_DIR, \"auto_insights.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33659ffa-5866-4088-b24d-98508860c7d1",
   "metadata": {},
   "source": [
    "#### 14) Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bacb9918-7b1b-4f7c-b684-04d2fc3252b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saram\\AppData\\Local\\Temp\\ipykernel_10372\\2079304669.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  trades.groupby(\"_s_bucket\")[\"_pnl\"]\n"
     ]
    }
   ],
   "source": [
    "if \"_s_bucket\" in trades.columns and \"_pnl\" in trades.columns:\n",
    "    bucket_stats = (\n",
    "        trades.groupby(\"_s_bucket\")[\"_pnl\"]\n",
    "        .agg([\"count\", \"mean\", \"median\", \"std\", \"sum\"])\n",
    "        .reset_index()\n",
    "        .sort_values(\"mean\", ascending=False)\n",
    "    )\n",
    "    bucket_stats.to_csv(os.path.join(OUTPUT_DIR, \"pnl_by_sentiment_bucket.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c751cb1-4079-4110-85b8-85d76fb5776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed. Outputs are in: assignment folder\n",
      "Key outputs saved: trades_enriched_sample.csv, daily_pnl_sent.csv, bucket_stats.csv, permutation_importance.csv (if model trained), models (if trained).\n"
     ]
    }
   ],
   "source": [
    "# Save enriched trades (sample) and aggregates\n",
    "trades.sample(n=min(5000, len(trades))).to_csv(os.path.join(OUTPUT_DIR,\"trades_enriched_sample.csv\"), index=False)\n",
    "bucket_stats.to_csv(os.path.join(OUTPUT_DIR,\"pnl_by_sentiment_bucket.csv\"), index=False)\n",
    "if 'acc_stats' in locals():\n",
    "    acc_stats.to_csv(os.path.join(OUTPUT_DIR,\"account_sent_return.csv\"), index=False)\n",
    "\n",
    "print(\"Completed. Outputs are in:\", OUTPUT_DIR)\n",
    "print(\"Key outputs saved: trades_enriched_sample.csv, daily_pnl_sent.csv, bucket_stats.csv, permutation_importance.csv (if model trained), models (if trained).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acee617-cc67-4ed5-9b03-c85b6efaac89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68012e85-3e34-4975-b161-ea66e2fccc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
